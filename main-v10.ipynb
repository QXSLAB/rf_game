{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "db = pdb.set_trace\n",
    "# Option 1.1 channel_state: number of users (easy)\n",
    "# Option 1.2 channel_state: interference power (hard)\n",
    "# Option 2.1 central learning and distributed executation (easy)\n",
    "# Option 2.2 distributed learing and distributed executation (hard)\n",
    "# Option 3.1 each agent has the same number of tasks (easy)\n",
    "# Option 3.2 each agent has different number of tasks (middle)\n",
    "# Option 3.3 each agent randomly generates tasks over time (hard)\n",
    "# Option 4.1 env give back reward every N step (waste bandwidth but easy)\n",
    "# Option 4.2 env only give back reward at last (save bandwidth but hard)\n",
    "# Option 5: adjust reward of communicate/tear/observe\n",
    "# Option 6.1: fully aware the env (do not need observe and easy)\n",
    "# Option 6.2: part aware the env (need to decide wether to observe and hard)\n",
    "# Option 7.1: action_args_pair = f(status) (easy but not scalable)\n",
    "#             number of output: num(action) x num(args)\n",
    "# Option 7.2: action, args = f(status) (hard but scalable)\n",
    "#             number of output: num(action) + num(args)\n",
    "# Option 8.1: use a step to do communicate schedule\n",
    "# Option 8.2: do not use a step to do communicate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_features, num_actions):\n",
    "        \"\"\"deep Q-learning network for testing algorithm\n",
    "           in_features: number of features of input.\n",
    "           num_actions: number of action-value, each corresponde to a action \n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "        \"\"\"Linear interpolation between initial_p and final_p over\n",
    "        schedule_timesteps. After this many timesteps pass final_p is\n",
    "        returned.\n",
    "        Parameters\n",
    "        ----------\n",
    "        schedule_timesteps: int\n",
    "            Number of timesteps for which to linearly anneal initial_p\n",
    "            to final_p\n",
    "        initial_p: float\n",
    "            initial output value\n",
    "        final_p: float\n",
    "            final output value\n",
    "        \"\"\"\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p            = final_p\n",
    "        self.initial_p          = initial_p\n",
    "        self.t = 0\n",
    "\n",
    "    def value(self):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        self.t += 1\n",
    "        fraction  = min(float(self.t) / self.schedule_timesteps, 1.0)\n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net = Net(N_STATES, N_ACTIONS).cuda()\n",
    "        self.target_net = Net(N_STATES, N_ACTIONS).cuda()\n",
    "        \n",
    "        self.schedular = LinearSchedule(EPSILON_STEPS, EPSILON)\n",
    "        \n",
    "        self.learn_step_counter = 0                                     \n",
    "        self.memory_counter = 0                                        \n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2)) \n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0).cuda()\n",
    "        # input only one sample\n",
    "        if np.random.uniform() > self.schedular.value():   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].cpu().numpy()[0]\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).cuda()\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)).cuda()\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]).cuda()\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).cuda()\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        # detach from graph, don't backpropagate\n",
    "        q_next = self.target_net(b_s_).detach()     \n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0]   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \"\"\"Currently no propogation effect is considered\n",
    "       The central station:\n",
    "       (1) only provide signaling channel and do not schedule at all\n",
    "       (2) monitor malicious users\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 channel_num,\n",
    "                 status_len,\n",
    "                 agent_num, \n",
    "                 task_num, \n",
    "                 max_steps,\n",
    "                 reward_interval):\n",
    "        \"\"\"channel_num: number of RF channels in the environment\n",
    "           status_len: use how many channel_state to construct env status\n",
    "           agent_num: number of agents to operate in this env\n",
    "           task_num: the number of tasks in each agent\n",
    "           max_steps: the number of steps in one epoch (ie. 50)\n",
    "                      when max_steps is reached game over and give reward\n",
    "           reward_interval: env give reward every reward_interval steps\n",
    "           \n",
    "           self.channel_state: log the channel user in this step\n",
    "           self.history: store channel occupancy of the past\n",
    "           self.step_index: log how many steps has gone\n",
    "           self.agent_list: log the agent in this environment\n",
    "           self.success_list: count success tasks of each agent\n",
    "           self.conflict_list: count conflict tasks of each agent\n",
    "           self.tmp_success_list: count success tasks over reward_interval\n",
    "           self.tmp_conflict_list: count conflict tasks over reward_interval\n",
    "        \"\"\"\n",
    "        self.channel_num = channel_num\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        \n",
    "        self.status_len = status_len\n",
    "        self.history = [[[] for i in range(self.channel_num)] \n",
    "                        for j in range(self.status_len)]\n",
    "        \n",
    "        self.agent_num = agent_num\n",
    "        self.task_num = task_num\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.step_index = 0\n",
    "        \n",
    "        self.reward_interval = reward_interval\n",
    "        \n",
    "        self.agent_list = []\n",
    "        self.success_list = []\n",
    "        self.conflict_list = []\n",
    "        \n",
    "        self.tmp_success_list = []\n",
    "        self.tmp_conflict_list = []\n",
    "        \n",
    "        self.dqn = DQN()\n",
    "        \n",
    "    # start to run the env with agents    \n",
    "    def game_on(self): \n",
    "        for ep in range(EPOCH_NUM):\n",
    "            self.reset()\n",
    "            for i in range(self.agent_num):\n",
    "                self.join(Agent(self))\n",
    "            while True:\n",
    "                done = self.step()\n",
    "                if self.dqn.memory_counter > MEMORY_CAPACITY:\n",
    "                    self.dqn.learn()\n",
    "                if done:\n",
    "                    if ep % PRINT_FREQ == 0: \n",
    "                        print(\"Epoch: {0:4d}\".format(ep), end='  ')\n",
    "                        self.report(verbose=False)\n",
    "                    break\n",
    "                \n",
    "    # reset the env            \n",
    "    def reset(self):\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        self.history = [[[] for i in range(self.channel_num)] \n",
    "                        for j in range(self.status_len)]\n",
    "        self.step_index = 0\n",
    "        self.agent_list = []\n",
    "        self.success_list = []\n",
    "        self.conflict_list = []\n",
    "        self.tmp_success_list = []\n",
    "        self.tmp_conflict_list = []\n",
    "    \n",
    "    # add agents into the env\n",
    "    def join(self, agent):\n",
    "        self.agent_list.append(agent)\n",
    "        self.success_list.append(0)\n",
    "        self.conflict_list.append(0)\n",
    "        \n",
    "        self.tmp_success_list.append(0)\n",
    "        self.tmp_conflict_list.append(0)\n",
    "    \n",
    "    # run one step\n",
    "    def step(self):\n",
    "        \"\"\"enter to next time step and initialize channel state\n",
    "           call step method of all agents\n",
    "           save channel state\n",
    "        \"\"\"\n",
    "        self.step_index += 1  \n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        for agent in self.agent_list:\n",
    "            agent.step()\n",
    "        #evaluate the transmission of each agent\n",
    "        self.ber()\n",
    "        self.history.append(self.channel_state)\n",
    "        done = 0 if self.step_index < self.max_steps else 1\n",
    "        return done\n",
    "\n",
    "    #evaluate ber\n",
    "    def ber(self):\n",
    "        \"\"\"count the success task and conflict task\n",
    "        \"\"\"\n",
    "        for state in self.channel_state:\n",
    "            if len(state) == 1:\n",
    "                agent = state[0]\n",
    "                index = self.agent_list.index(agent)\n",
    "                self.success_list[index] += 1\n",
    "                self.tmp_success_list[index] += 1\n",
    "            if len(state) > 1:\n",
    "                for agent in state:\n",
    "                    index = self.agent_list.index(agent)\n",
    "                    self.conflict_list[index] += 1\n",
    "                    self.tmp_conflict_list[index] += 1\n",
    "    \n",
    "    # return all agents\n",
    "    #spread communication message to all agents\n",
    "    def broadcast(self):\n",
    "        return self.agent_list\n",
    "\n",
    "    def propagation(self, channel_index, agent):\n",
    "        \"\"\"propagate the signal of a certain agent\"\"\"\n",
    "        self.channel_state[channel_index].append(agent)\n",
    "\n",
    "    def query(self, channel_index):\n",
    "        \"\"\"return agents which occupied the channel\n",
    "           which can be implemented by signal classification in experiment\n",
    "        \"\"\"\n",
    "        return self.history[-1][channel_index]\n",
    "\n",
    "    #Option 6.2\n",
    "    def sense(self):\n",
    "        \"\"\"return the number of channel users\n",
    "           which can be implemented by spectrum sensing in experiment\n",
    "        \"\"\"\n",
    "        return [len(l) for l in self.history[-1]]\n",
    "    \n",
    "    #Option 6.1\n",
    "    def status(self, flat):\n",
    "        \"\"\"return the status of the env\"\"\"\n",
    "        st = [[len(l) for l in self.history[-(j+1)]] \n",
    "                      for j in range(self.status_len)]\n",
    "        if flat:\n",
    "            stf = []\n",
    "            for l in st:\n",
    "                stf += l\n",
    "            return stf\n",
    "        else:\n",
    "            return st\n",
    "    \n",
    "    # Option 4.2\n",
    "    def get_reward(self, agent):\n",
    "        # The instructor is the environment (receiver),\n",
    "        # which can evaluate how good the agent is doing by checksum.\n",
    "        # The reward message is passed over signaling channel.\n",
    "        # Since the frequent receiver-to-agent interaction wastes bandwidth,\n",
    "        # the agent can only get back reward after its tasks are all finished\n",
    "        # or the maximum time step is reached.\n",
    "        \n",
    "        if self.step_index < self.max_steps-1:\n",
    "            return 0\n",
    "        if self.step_index == self.max_steps-1:\n",
    "            index = self.agent_list.index(agent)\n",
    "            return self.success_list[index]+self.conflict_list[index]\n",
    "        \n",
    "    # Option 4.1\n",
    "    def tmp_get_reward(self, agent):\n",
    "\n",
    "        if self.step_index % self.max_steps == 0 \\\n",
    "           or self.step_index == self.max_steps:\n",
    "            \n",
    "            index = self.agent_list.index(agent)\n",
    "            reward = (self.tmp_success_list[index]\n",
    "                      +self.tmp_conflict_list[index])\n",
    "            self.tmp_success_list[index] = 0\n",
    "            self.tmp_conflict_list[index] = 0\n",
    "            return reward\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def report(self, verbose):\n",
    "        \"\"\"report status of all agent\"\"\"\n",
    "        if verbose:\n",
    "            for index,agent in enumerate(self.agent_list):\n",
    "                print(\"success:{0:<4d}, \"\n",
    "                      \"conflict:{1:<4d}, \"\n",
    "                      \"remain:{2:<4d}, \"\n",
    "                      \"channel:{3:<3d}, \"\n",
    "                      \"reward: {4:<4.4f}\".format(\n",
    "                          self.success_list[index],\n",
    "                          self.conflict_list[index],\n",
    "                          agent.report()[0],\n",
    "                          sum(agent.report()[1]),\n",
    "                          agent.report()[2],\n",
    "                      ))\n",
    "            print(\"success rate: %f\"\n",
    "                  % (sum(self.success_list)/(self.task_num*self.agent_num)))\n",
    "            #visulize spectrum use and conflict\n",
    "            spec_his = np.array([[len(x) for x in y] for y in self.history])\n",
    "            plt.matshow(spec_his.T)\n",
    "            plt.colorbar()\n",
    "        else:\n",
    "            r = []\n",
    "            for index,agent in enumerate(self.agent_list):\n",
    "                r.append(agent.report()[2])\n",
    "                print(\"{0:4.2f}\".format(r[-1]), end=\", \")\n",
    "            r = np.array(r)\n",
    "            print(\"mean:{0:4.2f}, std:{1:4.2f}\".format(np.mean(r),np.std(r)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"at the beginning, random trasmit\n",
    "       then mainly use coordinate to gain channel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"env: the environment to operate in\n",
    "           self.channels: established channels (coordinated with receiver)\n",
    "                          0-not established, 1-established\n",
    "           self.part_state: observed channel states\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.task_num = self.env.task_num\n",
    "        self.channels = [0 for i in range(self.env.channel_num)]\n",
    "        self.part_state = []\n",
    "        \n",
    "        self.operation_dict = {1:self.rest,\n",
    "                               2:self.observe,\n",
    "                               3:self.transmit,\n",
    "                               4:self.tear,\n",
    "                               5:self.establish,\n",
    "                               6:self.communicate}\n",
    "        \n",
    "        self.dqn = self.env.dqn\n",
    "        \n",
    "        # action is (operation, channel_index)\n",
    "        action_id = 0\n",
    "        self.action_dict = {}\n",
    "        for _, operation in self.operation_dict.items():\n",
    "            args_num = operation.__code__.co_argcount\n",
    "            if args_num == 1:\n",
    "                self.action_dict[action_id] = (operation,0)\n",
    "                action_id += 1\n",
    "            else:\n",
    "                for i in range(self.env.channel_num):\n",
    "                    self.action_dict[action_id] = (operation,i)\n",
    "                    action_id += 1\n",
    "        \n",
    "        self.status_curve = []\n",
    "        self.action_curve = []\n",
    "        self.reward_curve = []\n",
    "    \n",
    "    # Option 8.2\n",
    "    def step(self):\n",
    "        \"\"\"interact with environment and agents within in one time step\n",
    "           using Q-learning to decide whether to tear channel when job finished\n",
    "        \"\"\"\n",
    "        self.reward_curve.append(0)\n",
    "        if self.task_num > 0:\n",
    "            # job has not finished at last step, punish for the delay\n",
    "            # since we want to finish tasks as soon as possible\n",
    "            self.reward_curve[-1] -= 0.5\n",
    "        \n",
    "#         # TODO: Q learning to choose operation (function and args)\n",
    "#         # func: rest,transmit,establish,tear,observe,communicate\n",
    "#         # args: channel_index\n",
    "#         # channel_index: ie. 0~10\n",
    "#         # status: task_num,channels,part_state\n",
    "#         # find f: function, channel_index, target_agent_id = f(status)\n",
    "        \n",
    "        s = [self.task_num] + self.channels + self.env.status(flat=True)\n",
    "        self.status_curve.append(s)\n",
    "        a = self.dqn.choose_action(s)\n",
    "#         a = random.randint(0, 32)\n",
    "        operation, channel_index = self.action_dict[a]\n",
    "        operation(channel_index)\n",
    "        self.action_curve.append(a)\n",
    "        \n",
    "#         operation_index = random.randint(1,6)\n",
    "#         channel_index = random.randint(0,self.env.channel_num-1)\n",
    "#         self.operation_dict[operation_index](channel_index)\n",
    "\n",
    "        self.reward_curve[-1] += self.env.tmp_get_reward(self)\n",
    "        if len(self.status_curve)>2:\n",
    "            self.dqn.store_transition(self.status_curve[-2],\n",
    "                                      self.action_curve[-2],\n",
    "                                      self.reward_curve[-2],\n",
    "                                      self.status_curve[-1],\n",
    "                                     )\n",
    "\n",
    "    def rest(self, *args):\n",
    "        \"\"\"reset in this time step and do nothing\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def transmit(self, *args):\n",
    "        \"\"\"transmit using all established channels\n",
    "           TODO: choose channel based on certain policy\n",
    "        \"\"\"\n",
    "        for index, ocp in enumerate(self.channels):\n",
    "            if self.task_num == 0: return\n",
    "            if ocp:\n",
    "                self.env.propagation(index, self)\n",
    "                self.task_num -= 1\n",
    "\n",
    "    def establish(self, channel_index, *args):\n",
    "        \"\"\"establish a new channel by coordinate with receiver\n",
    "           channel_index: the index of channel to occupy\n",
    "        \"\"\"\n",
    "        if self.channels[channel_index] == 0:\n",
    "            self.channels[channel_index] = 1\n",
    "            # Since expand operation need signaling bandwidth to coordinate\n",
    "            self.reward_curve[-1] -= 1\n",
    "\n",
    "    def tear(self, channel_index, *args):\n",
    "        \"\"\"tear down a channel by coordinate with receiver\n",
    "           channel_index: the index of channel to release\n",
    "        \"\"\"\n",
    "        if self.channels[channel_index] == 1:\n",
    "            self.channels[channel_index] = 0\n",
    "            # Shrink operation do not need signaling bandwidth to coordinate (0)\n",
    "            # use the current data channel to trasmit tear down signal\n",
    "            # Shrink operation ficilitate collabaration(+1)\n",
    "            # decrease the possibility to conflict\n",
    "            self.reward_curve[-1] += 1\n",
    "\n",
    "    def observe(self, *args):\n",
    "        \"\"\"observe the channel usages\n",
    "           TODO: may directly return the channel with highest availability\n",
    "           TODO: in this stage, we let Q-learning find the candidate channel\n",
    "        \"\"\"\n",
    "        state = copy.deepcopy(self.env.sense())\n",
    "        self.part_state.append(state.append(self.env.step_index)) \n",
    "        # Since observe operation need energy to detect occupancy\n",
    "        self.reward_curve[-1] -= 0.2\n",
    "        # TODO find the channel with highest availability (least occupied)\n",
    "    \n",
    "    # abandon p2p communicate\n",
    "    # reason 1: p2p communicate -> feedback are not stable\n",
    "    # ie. A:0.1, B:0.3, C:0.2, multiple communicate may overwrite each other\n",
    "    # reason 2: identify result is not reliable\n",
    "    # since you do not exactly know whether the agent is still using the channel\n",
    "    # so use broadcast communicate\n",
    "    def communicate(self, channel_index, *args):\n",
    "        \"\"\"communicate with other agents over signaling channel\n",
    "           the priority and desired channel is broadcast to all other agents           target: the target agent to communicate with\n",
    "           protocal: (1) exchange priority score to collabration\n",
    "                     (2) protect agents using different protocals\n",
    "           since can not collaborate with them means can not use the channel\n",
    "           and you will be interferenced when you do\n",
    "        \"\"\"\n",
    "        # Since communicate need signaling bandwidth to coordinate (-1)\n",
    "        self.reward_curve[-1] -= 1\n",
    "        score = self.priority()\n",
    "        for agent in self.env.broadcast():\n",
    "            if agent.insist(channel_index, score): break\n",
    "        else:\n",
    "            self.establish(channel_index)\n",
    "            \n",
    "    # TODO: log communicate result and use Q-learning decide what to do next\n",
    "    # Infeasible： since different agents are difficult to reach consensus\n",
    "\n",
    "    def insist(self, channel_index, score):\n",
    "        \"\"\"schedule tear down operation if score is higher\n",
    "           return the priority score of this agent\n",
    "        \"\"\"\n",
    "        if self.channels[channel_index] == 0:\n",
    "            return False\n",
    "        if score > self.priority():\n",
    "            self.tear(channel_index)\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def priority(self):\n",
    "        \"\"\"calculate priority score, or loss of the agent\n",
    "           possible metric is to combine tasks num and self.reward_curve\n",
    "           need to add constrict over score to avoid malicious deception\n",
    "           通过（1）设备入网审查；（2）监测加入时间和后续发送数，保证score真实性\n",
    "        \"\"\"\n",
    "        return self.task_num - sum(self.channels) - sum(self.reward_curve)\n",
    "    \n",
    "    def report(self):\n",
    "        return self.task_num, self.channels, sum(self.reward_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "CHANNEL_NUM = 10\n",
    "STATUS_LEN = 5\n",
    "AGENT_NUM = 5\n",
    "TASK_NUM = 100\n",
    "MAX_STEPS = 50\n",
    "REWARD_INTERVAL = 10\n",
    "N_ACTIONS = 3 + 3*CHANNEL_NUM\n",
    "N_STATES = CHANNEL_NUM*STATUS_LEN + CHANNEL_NUM + 1\n",
    "\n",
    "EPOCH_NUM = 1000\n",
    "PRINT_FREQ = 50\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.1              # greedy policy\n",
    "EPSILON_STEPS = int(0.8*AGENT_NUM*MAX_STEPS*EPOCH_NUM)\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = int(0.1*AGENT_NUM*MAX_STEPS*EPOCH_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment(channel_num=CHANNEL_NUM,\n",
    "                 status_len=STATUS_LEN,\n",
    "                 agent_num=AGENT_NUM, \n",
    "                 task_num=TASK_NUM, \n",
    "                 max_steps=MAX_STEPS,\n",
    "                 reward_interval=REWARD_INTERVAL)\n",
    "e.game_on()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python fastaiv1py37",
   "language": "python",
   "name": "fastaiv1py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
