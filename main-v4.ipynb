{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Option 1.1 channel_state: number of users\n",
    "# Option 1.2 channel_state: interference power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \"\"\"Currently no propogation effect is considered\n",
    "       The central station only provide signaling channel\n",
    "       and do not schedule at all\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_num, max_steps):\n",
    "        \"\"\"channel_num: number of RF channels in the environment\n",
    "           max_steps: the number of steps in one epoch (ie. 50)\n",
    "                      when max_steps is reached game over and give reward\n",
    "           self.channel_state: record the channel occupancy in this step\n",
    "           self.history: store channel occupancy of the past\n",
    "           self.step: log how many steps has gone\n",
    "           self.agent_list: log the agent in this environment\n",
    "           self.agent_list: record the reward of each agent\n",
    "        \"\"\"\n",
    "        self.channel_num = channel_num\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        self.history = []\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.step = 0\n",
    "        \n",
    "        self.agent_list = []\n",
    "        self.reward_list = []\n",
    "        \n",
    "    def join(self, agent):\n",
    "        self.agent_list.append(agent)\n",
    "        self.reward_list.append([])\n",
    "        \n",
    "    def one_action_step(self):\n",
    "        \"\"\"record steps and save last channel state\n",
    "           enter to next time step and initialize channel state\n",
    "           call one_action_step method of agents in agent list\n",
    "        \"\"\"\n",
    "        self.step += 1\n",
    "        self.history.append(self.channel_state)\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        for a in agent_list:\n",
    "            a.one_time_step()\n",
    "        #evaluate the reward of each agent\n",
    "        ber()\n",
    "    \n",
    "    def ber(self):\n",
    "        \"\"\"reward: success transmission (1/task), conflict trans (-1/task)\n",
    "        \"\"\"\n",
    "        for state in enumerate(self.channel_state):\n",
    "            if len(state) == 1:\n",
    "                agent = state[0]\n",
    "                index = self.agent_list.index(agent)\n",
    "                self.reward_list[index] += 1\n",
    "            if len(state) > 1:\n",
    "                for agent in state:\n",
    "                    index = self.agent_list.index(agent)\n",
    "                    self.reward_list[index] -= 1\n",
    "\n",
    "    def propagation(self, channel_index, agent):\n",
    "        \"\"\"propagate the signal of a certain agent\"\"\"\n",
    "        self.channel_state[channel_index].append(agent)\n",
    "\n",
    "    def query(self, channel_index):\n",
    "        \"\"\"provide identification result\n",
    "           return the list of agents occupying the certain channel\n",
    "        \"\"\"\n",
    "        return self.history[-1][channel_index]\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\"provid observation result\n",
    "           return the number of channel users \n",
    "        \"\"\"\n",
    "        last_state = self.history[-1]\n",
    "        return [len(l) for l in last_state]\n",
    "\n",
    "    def get_reward(self, agent):\n",
    "        # The instructor is the environment (receiver),\n",
    "        # which can evaluate how good the agent is doing by checksum.\n",
    "        # The reward message is passed over signaling channel.\n",
    "        # Since the frequent receiver-to-agent interaction wastes bandwidth,\n",
    "        # the agent can only get back reward after its tasks are all finished\n",
    "        # or the maximum time step is reached.\n",
    "        if self.step < self.max_steps:\n",
    "            return 0\n",
    "        if self.step == self.max_steps:\n",
    "            index = self.agent_list.index(agent)\n",
    "            return self.reward_list[index]\n",
    "        if self.step > self.max_steps:\n",
    "            raise RuntimeError('maximum steps reached and game over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, env, task_num):\n",
    "        \"\"\"env: the environment to operate in\n",
    "           task_num: number of task to be transmitted, ie. 100\n",
    "           self.channels: established channels (coordinated with receiver)\n",
    "           self.part_state: observed channel states\n",
    "           self.part_agent: identified agents in each channel\n",
    "           self.agent_id: log agents apeared\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.task_num = task_num\n",
    "        self.channels = []\n",
    "        self.part_state = []\n",
    "        self.part_agent = [[] for i in range(self.env.channel_num)]\n",
    "        self.agent_id = {}\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.action_queue = []\n",
    "        self.args_queue = []\n",
    "\n",
    "    def one_action_step(self):\n",
    "        \"\"\"interact with environment and agents within in one time step\n",
    "           using Q-learning to decide whether to tear channel when job finished\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.task_num > 0:\n",
    "            # job has not finished at last step, punish for the delay\n",
    "            # since we want to finish tasks as soon as possible\n",
    "            self.reward -= 0.5\n",
    "        \n",
    "        if len(self.action_queue)>0：\n",
    "            # execute the actions scheduled by communication\n",
    "            # since communicate may occur multiple times\n",
    "            action = self.action_queue[-1]\n",
    "            args = self.args_queue[-1]\n",
    "            action(args)\n",
    "            # TODO: log the communicate result and use Q-learning to decide\n",
    "            # what to do\n",
    "        else:\n",
    "            # TODO: Q learning to choose operation (function and args)\n",
    "            # func: rest,transmit,establish,tear,observe,identify,communicate\n",
    "            # args: channel_index, target_agent\n",
    "            # channel_index: 0~channel_num\n",
    "            # target_agent: self.agent_id.get(target_agent_id,'null')\n",
    "            # status: task_num,channels,part_state,part_agent\n",
    "            # find f: function, channel_index, target_agent_id = f(status)\n",
    "        \n",
    "        # get reward from environment\n",
    "        self.reward += self.env.get_reward(self)\n",
    "         \n",
    "\n",
    "    def rest(self, *args):\n",
    "        \"\"\"reset in this time step and do nothing\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def transmit(self, *args):\n",
    "        \"\"\"transmit using all established channels\n",
    "           TODO: choose channel based on certain policy\n",
    "        \"\"\"\n",
    "        for index in self.channels:\n",
    "            if self.task_num == 0: return\n",
    "            self.env.propagation(index, self)\n",
    "            self.task_num -= 1       \n",
    "\n",
    "    def establish(self, channel_index, *args):\n",
    "        \"\"\"establish a new channel by coordinate with receiver\n",
    "           channel_index: the index of channel to occupy\n",
    "        \"\"\"\n",
    "        if channel_index not in self.channels:\n",
    "            self.channels.append(channel_index)\n",
    "            # Since expand operation need signaling bandwidth to coordinate\n",
    "            self.reward -= 1\n",
    "\n",
    "    def tear(self, channel_index, *args):\n",
    "        \"\"\"tear down a channel by coordinate with receiver\n",
    "           channel_index: the index of channel to release\n",
    "        \"\"\"\n",
    "        if channel_index in self.channels:\n",
    "            self.channels.remove(channel_index)\n",
    "            # Shrink operation need signaling bandwidth to coordinate (-1)\n",
    "            # Shrink operation save resource in agent and receiver (+1)\n",
    "            # Shrink operation ficilitate collabaration(+1)\n",
    "            self.reward += 2\n",
    "\n",
    "    def observe(self, *args):\n",
    "        \"\"\"observe the channel usages\n",
    "           save number of users in each channel\n",
    "           TODO: may directly return the channel with highest availability\n",
    "           TODO: in this stage, we let Q-learning find the candidate channel\n",
    "        \"\"\"\n",
    "        self.part_state.append(self.env.report()) \n",
    "        # Since observe operation need energy to detect occupancy\n",
    "        self.reward -= 0.5\n",
    "        # TODO find the channel with highest availability (least occupied)\n",
    "        \n",
    "    def identify(self, channel_index, *args):\n",
    "        \"\"\"identifying the agent occupying the channel \n",
    "           by classifying signal (implemented as environment query)\n",
    "           save user occupying the perticular channel\n",
    "           channel_index: the index of the channel to identify\n",
    "        \"\"\"\n",
    "        users = self.env.query(channel_index)\n",
    "        for u in users:\n",
    "            ags = filter(lambda x: u is x[1], self.agent_id.items())\n",
    "            if len(ags) ==0:\n",
    "                id_num = max(agent_id.keys())+1\n",
    "                self.agent_id[id_num] = u\n",
    "                self.part_agent(channel_index).append(id_num)\n",
    "            else:\n",
    "                self.part_agent(channel_index).append(ags[0])\n",
    "        # Since identify signal need energy\n",
    "        self.reward -= 0.5\n",
    "\n",
    "    def communicate(self, channel_index, target_agent, *args):\n",
    "        \"\"\"communicate with destinate agent over signaling channel\n",
    "           say desired channel, exchange priority and schedule establish action\n",
    "           target: the target agent to communicate with\n",
    "           protocal: (1) exchange priority score to collabration\n",
    "                     (2) protect agents using different protocals\n",
    "           since can not collaborate with them means can not use the channel\n",
    "           and you will be interferenced when you do\n",
    "        \"\"\"\n",
    "        score = priority()\n",
    "        target_score = target_agent.feedback(channel_index, score)\n",
    "        if score > target_score:\n",
    "            self.action_queue.append(self.establish)\n",
    "            self.args_queue.append((channel_index,))\n",
    "        # Since communicate need signaling bandwidth to coordinate (-1)\n",
    "        # Since communicate ficilitate information sharing\n",
    "        self.reward -= 1\n",
    "        \n",
    "    def feedback(self, channel_index, score):\n",
    "        \"\"\"schedule tear down action if score is higher\n",
    "           return the priority score of this agent\n",
    "        \"\"\"\n",
    "        if score > priority() and channel_index in self.channels:\n",
    "            self.action_queue.append(self.tear)\n",
    "            self.args_queue.append((channel_index,))\n",
    "        return priority()\n",
    "        \n",
    "    def priority(self):\n",
    "        \"\"\"calculate priority score, or loss of the agent\n",
    "           possible metric is to combine tasks num and self.reward\n",
    "           need to add constrict over score to avoid malicious deception\n",
    "           通过（1）设备入网审查；（2）监测加入时间和后续发送数，保证score真实性\n",
    "        \"\"\"\n",
    "        return -self.task_num + self.reward\n",
    "    \n",
    "    def report(self):\n",
    "        return self.task_num, self.channels, self.reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python fastaiv1py37",
   "language": "python",
   "name": "fastaiv1py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
