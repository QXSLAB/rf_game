{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Option 1.1 channel_state: number of users\n",
    "# Option 1.2 channel_state: interference power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \"\"\"Currently no propogation effect is considered\n",
    "       The central station:\n",
    "       (1) only provide repeating signaling channel and do not schedule at all\n",
    "       (2) monitor malicious users\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_num, max_steps):\n",
    "        \"\"\"channel_num: number of RF channels in the environment\n",
    "           max_steps: the number of steps in one epoch (ie. 50)\n",
    "                      when max_steps is reached game over and give reward\n",
    "           self.channel_state: record the channel occupancy in this step\n",
    "           self.history: store channel occupancy of the past\n",
    "           self.step: log how many steps has gone\n",
    "           self.agent_list: log the agent in this environment\n",
    "           self.agent_list: record the reward of each agent\n",
    "        \"\"\"\n",
    "        self.channel_num = channel_num\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        self.history = []\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.step = 0\n",
    "        \n",
    "        self.agent_list = []\n",
    "        self.reward_list = []\n",
    "        \n",
    "    def join(self, agent):\n",
    "        self.agent_list.append(agent)\n",
    "        self.reward_list.append([])\n",
    "        \n",
    "    def one_action_step(self):\n",
    "        \"\"\"record steps and save last channel state\n",
    "           enter to next time step and initialize channel state\n",
    "           call one_action_step method of agents in agent list\n",
    "        \"\"\"\n",
    "        self.step += 1\n",
    "        self.history.append(self.channel_state)\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        for agent in agent_list:\n",
    "            agent.one_time_step()\n",
    "        #evaluate the reward of each agent\n",
    "        ber()\n",
    "    \n",
    "    def ber(self):\n",
    "        \"\"\"reward: success transmission (1/task), conflict trans (-1/task)\n",
    "        \"\"\"\n",
    "        for state in enumerate(self.channel_state):\n",
    "            if len(state) == 1:\n",
    "                agent = state[0]\n",
    "                index = self.agent_list.index(agent)\n",
    "                self.reward_list[index] += 1\n",
    "            if len(state) > 1:\n",
    "                for agent in state:\n",
    "                    index = self.agent_list.index(agent)\n",
    "                    self.reward_list[index] -= 1\n",
    "    \n",
    "    def broadcast(self):\n",
    "        return self.agent_list\n",
    "\n",
    "    def propagation(self, channel_index, agent):\n",
    "        \"\"\"propagate the signal of a certain agent\"\"\"\n",
    "        self.channel_state[channel_index].append(agent)\n",
    "\n",
    "    def query(self, channel_index):\n",
    "        \"\"\"provide identification result\n",
    "           return the list of agents occupying the certain channel\n",
    "        \"\"\"\n",
    "        return self.history[-1][channel_index]\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\"provid observation result\n",
    "           return the number of channel users \n",
    "        \"\"\"\n",
    "        last_state = self.history[-1]\n",
    "        return [len(l) for l in last_state]\n",
    "\n",
    "    def get_reward(self, agent):\n",
    "        # The instructor is the environment (receiver),\n",
    "        # which can evaluate how good the agent is doing by checksum.\n",
    "        # The reward message is passed over signaling channel.\n",
    "        # Since the frequent receiver-to-agent interaction wastes bandwidth,\n",
    "        # the agent can only get back reward after its tasks are all finished\n",
    "        # or the maximum time step is reached.\n",
    "        if self.step < self.max_steps:\n",
    "            return 0\n",
    "        if self.step == self.max_steps:\n",
    "            index = self.agent_list.index(agent)\n",
    "            return self.reward_list[index]\n",
    "        if self.step > self.max_steps:\n",
    "            raise RuntimeError('maximum steps reached and game over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"at the beginning, random trasmit\n",
    "       then mainly use coordinate to gain channel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, task_num):\n",
    "        \"\"\"env: the environment to operate in\n",
    "           task_num: number of task to be transmitted, ie. 100\n",
    "           self.channels: established channels (coordinated with receiver)\n",
    "           self.part_state: observed channel states\n",
    "           self.part_agent: identified agents in each channel\n",
    "           self.agent_id: log agents apeared\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.task_num = task_num\n",
    "        self.channels = []\n",
    "        self.part_state = []\n",
    "        self.agent_id = {}\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.action_queue = []\n",
    "        self.args_queue = []\n",
    "\n",
    "    def one_action_step(self):\n",
    "        \"\"\"interact with environment and agents within in one time step\n",
    "           using Q-learning to decide whether to tear channel when job finished\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.task_num > 0:\n",
    "            # job has not finished at last step, punish for the delay\n",
    "            # since we want to finish tasks as soon as possible\n",
    "            self.reward -= 0.5\n",
    "        \n",
    "        if len(self.action_queue)>0：\n",
    "            # execute the actions scheduled by communication\n",
    "            # since communicate may occur multiple times\n",
    "            action = self.action_queue[-1]\n",
    "            args = self.args_queue[-1]\n",
    "            action(args)\n",
    "            # TODO: log the communicate result and use Q-learning to decide\n",
    "            # what to do\n",
    "        else:\n",
    "            # TODO: Q learning to choose operation (function and args)\n",
    "            # func: rest,transmit,establish,tear,observe,communicate\n",
    "            # args: channel_index\n",
    "            # channel_index: ie. 0~10\n",
    "            # status: task_num,channels,part_state\n",
    "            # find f: function, channel_index, target_agent_id = f(status)\n",
    "        \n",
    "        # get reward from environment\n",
    "        self.reward += self.env.get_reward(self)\n",
    "\n",
    "    def rest(self, *args):\n",
    "        \"\"\"reset in this time step and do nothing\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def transmit(self, *args):\n",
    "        \"\"\"transmit using all established channels\n",
    "           TODO: choose channel based on certain policy\n",
    "        \"\"\"\n",
    "        for index in self.channels:\n",
    "            if self.task_num == 0: return\n",
    "            self.env.propagation(index, self)\n",
    "            self.task_num -= 1       \n",
    "\n",
    "    def establish(self, channel_index, *args):\n",
    "        \"\"\"establish a new channel by coordinate with receiver\n",
    "           channel_index: the index of channel to occupy\n",
    "        \"\"\"\n",
    "        if channel_index not in self.channels:\n",
    "            self.channels.append(channel_index)\n",
    "            # Since expand operation need signaling bandwidth to coordinate\n",
    "            self.reward -= 1\n",
    "\n",
    "    def tear(self, channel_index, *args):\n",
    "        \"\"\"tear down a channel by coordinate with receiver\n",
    "           channel_index: the index of channel to release\n",
    "        \"\"\"\n",
    "        if channel_index in self.channels:\n",
    "            self.channels.remove(channel_index)\n",
    "            # Shrink operation do not need signaling bandwidth to coordinate (0)\n",
    "            # use the current data channel to trasmit tear down signal\n",
    "            # Shrink operation ficilitate collabaration(+1)\n",
    "            # decrease the possibility to conflict good for myself\n",
    "            self.reward += 1\n",
    "\n",
    "    def observe(self, *args):\n",
    "        \"\"\"observe the channel usages\n",
    "           TODO: may directly return the channel with highest availability\n",
    "           TODO: in this stage, we let Q-learning find the candidate channel\n",
    "        \"\"\"\n",
    "        self.part_state.append(self.env.report()) \n",
    "        # Since observe operation need energy to detect occupancy\n",
    "        self.reward -= 0.2\n",
    "        # TODO find the channel with highest availability (least occupied)\n",
    "    \n",
    "    # abandon p2p communicate\n",
    "    # reason 1: p2p communicate -> feedback are not stable\n",
    "    # ie. A:0.1, B:0.3, C:0.2, multiple communicate may overwrite each other\n",
    "    # reason 2: identify result is not reliable\n",
    "    # since you do not exactly know whether the agent is still using the channel\n",
    "    # so use broadcast communicate\n",
    "    def communicate(self, channel_index, *args):\n",
    "        \"\"\"communicate with other agents over signaling channel\n",
    "           the priority and desired channel is broadcast to all other agents           target: the target agent to communicate with\n",
    "           protocal: (1) exchange priority score to collabration\n",
    "                     (2) protect agents using different protocals\n",
    "           since can not collaborate with them means can not use the channel\n",
    "           and you will be interferenced when you do\n",
    "        \"\"\"\n",
    "        # Since communicate need signaling bandwidth to coordinate (-1)\n",
    "        self.reward -= 1\n",
    "        score = priority()\n",
    "        for agent in self.env.broadcast():\n",
    "            if not agent.insist(channel_index, score): break\n",
    "        else:\n",
    "            return\n",
    "        self.action_queue.append(self.establish)\n",
    "        self.args_queue.append((channel_index,))  \n",
    "\n",
    "    def insist(self, channel_index, score):\n",
    "        \"\"\"schedule tear down action if score is higher\n",
    "           return the priority score of this agent\n",
    "        \"\"\"\n",
    "        if channel_index not in self.channels:\n",
    "            return True\n",
    "        if score > self.priority():\n",
    "            self.action_queue.append(self.tear)\n",
    "            self.args_queue.append((channel_index,))\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def priority(self):\n",
    "        \"\"\"calculate priority score, or loss of the agent\n",
    "           possible metric is to combine tasks num and self.reward\n",
    "           need to add constrict over score to avoid malicious deception\n",
    "           通过（1）设备入网审查；（2）监测加入时间和后续发送数，保证score真实性\n",
    "        \"\"\"\n",
    "        return self.task_num - len(self.channels) - self.reward\n",
    "    \n",
    "    def report(self):\n",
    "        return self.task_num, self.channels, self.reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python fastaiv1py37",
   "language": "python",
   "name": "fastaiv1py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
