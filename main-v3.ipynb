{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Option 1.1 channel_state: number of users\n",
    "# Option 1.2 channel_state: interference power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \"\"\"Currently no propogation effect is considered\"\"\"\n",
    "\n",
    "    def __init__(self, channel_num):\n",
    "        \"\"\"channel_num: number of RF channels in the environment\"\"\"\n",
    "        self.channel_num = channel_num\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        self.history = []\n",
    "        \n",
    "    def one_action_step(self):\n",
    "        \"\"\"save last channel state\n",
    "           enter to next time step and initialize channel state\n",
    "        \"\"\"\n",
    "        self.history.append(self.channel_state)\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "\n",
    "    def join(self, channel_index, agent):\n",
    "        \"\"\"accept transmitting of a certain agent\"\"\"\n",
    "        self.channel_state[channel_index].append(agent)\n",
    "\n",
    "    def query(self, channel_index):\n",
    "        \"\"\"provide identification result\n",
    "           return the list of agents occupying the certain channel\n",
    "        \"\"\"\n",
    "        return self.history[-1][channel_index]\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\"provid observation result\n",
    "           return the number of channel users \n",
    "        \"\"\"\n",
    "        last_state = self.history[-1]\n",
    "        return [len(l) for l in last_state]\n",
    "\n",
    "    def get_reward(self):\n",
    "        # TODO Give success (1/task), conflict(-1/task)\n",
    "        # The instructor is the environment (receiver),\n",
    "        # which can evaluate how good the agent is doing by checksum.\n",
    "        # The reward message is passed over signaling channel.\n",
    "        # Since the frequent receiver to agent interaction wastes bandwidth,\n",
    "        # the agent can only get back reward after its tasks are all finished\n",
    "        # or the maximum time step is reached.\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, env, task_num):\n",
    "        \"\"\"task_num: number of task to be transmitted\n",
    "           env: the environment to operate in\n",
    "           self.channels: established channels with receiver\n",
    "           self.part_state: observed channel states\n",
    "           self.part_agent: identified agents in each channel\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.task_num = task_num\n",
    "        self.channels = set()\n",
    "        self.part_state = []\n",
    "        self.part_agent = [[] for i in range(self.env.channel_num)]\n",
    "        self.agent_id = {}\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.action_queue = []\n",
    "        self.args_queue = []\n",
    "\n",
    "    def one_action_step(self):\n",
    "        \"\"\"interact with environment and other agents in one time step\n",
    "        \"\"\"\n",
    "        if self.task_num == 0:\n",
    "            # all job just finished and get back the final reward from env\n",
    "            self.reward += self.env.get_reward()\n",
    "            # mark all job has finished\n",
    "            self.task_num = -1\n",
    "            return\n",
    "        if self.task_num == -1:\n",
    "            # all job has finished and begin to tear down channels\n",
    "            if len(self.channels)>0:\n",
    "                index = self.channels.pop()\n",
    "                self.tear(index)            \n",
    "            return \n",
    "        if len(self.action_queue)>0：\n",
    "            # execute the actions scheduled by communication\n",
    "            action = self.action_queue.pop(0)\n",
    "            args = self.args_queue.pop(0)\n",
    "            action(args)\n",
    "            # job has not finished at last step, punish for the delay\n",
    "            # since we want to finish tasks as soon as possible\n",
    "            self.reward -= 1\n",
    "            return\n",
    "        \n",
    "        # TODO: Q learning to choose operation (function and args)\n",
    "        # function: rest,transmit,establish,tear,observe,identify,communicate\n",
    "        # args: channel_index, target_agent\n",
    "        # channel_index: 0~channel_num\n",
    "        # target_agent: self.agent_id.get(target_agent_id,'null')\n",
    "        # status: task_num,channels,part_state,part_agent\n",
    "        # find f: function, channel_index, target_agent_id = f(status)\n",
    "         \n",
    "\n",
    "    def rest(self, *args):\n",
    "        \"\"\"reset in this time step and do nothing\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def transmit(self, *args):\n",
    "        \"\"\"transmit using the established channels\n",
    "           TODO: choose channel based on certain policy\n",
    "        \"\"\"\n",
    "        for index in self.channels:\n",
    "            if self.task_num == 0: break\n",
    "            self.env.join(index, self)\n",
    "            self.task_num -= 1       \n",
    "\n",
    "    def establish(self, channel_index, *args):\n",
    "        \"\"\"establish a new channel by coordinate with receiver\n",
    "           channel_index: the index of channel to occupy\n",
    "        \"\"\"\n",
    "        if channel_index not in self.channels:\n",
    "            self.channels.add(channel_index)\n",
    "            # Since expand operation need signaling bandwidth to coordinate\n",
    "            self.reward -= 1\n",
    "\n",
    "    def tear(self, channel_index, *args):\n",
    "        \"\"\"tear down a channel by coordinate with receiver\n",
    "           channel_index: the index of channel to release\n",
    "        \"\"\"\n",
    "        if channel_index in self.channels:\n",
    "            self.channels.remove(channel_index)\n",
    "            # Shrink operation need signaling bandwidth to coordinate (-1)\n",
    "            # Shrink operation save resource in agent and receiver (+1)\n",
    "            # Shrink operation ficilitate collabaration(+1)\n",
    "            self.reward += 2\n",
    "\n",
    "    def observe(self, *args):\n",
    "        \"\"\"observe the channel usages\n",
    "           save number of users in each channel\n",
    "           TODO: may can add find channel with highest availability\n",
    "           TODO: in this stage, we let Q-learning find the candidate channel\n",
    "        \"\"\"\n",
    "        self.part_state.append(self.env.report()) \n",
    "        # Since observe operation need energy to detect occupancy\n",
    "        self.reward -= 0.5\n",
    "        # TODO find the channel with highest availability (least occupied)\n",
    "        \n",
    "    def identify(self, channel_index, *args):\n",
    "        \"\"\"identifying the agent occupying the channel \n",
    "           by classifying signal (implemented as environment query)\n",
    "           save user occupying the perticular channel\n",
    "           channel_index: the index of the channel to identify\n",
    "        \"\"\"\n",
    "        users = self.env.query(channel_index)\n",
    "        for u in users:\n",
    "            ags = filter(lambda x: u is x[1], self.agent_id.items())\n",
    "            if len(ags) ==0:\n",
    "                id_num = max(agent_id.keys())+1\n",
    "                self.agent_id[id_num] = u\n",
    "                self.part_agent(channel_index).append(id_num)\n",
    "            else:\n",
    "                self.part_agent(channel_index).append(ags[0])\n",
    "        # Since identify signal need energy\n",
    "        self.reward -= 0.5\n",
    "\n",
    "    def communicate(self, channel_index, target_agent, *args):\n",
    "        \"\"\"communicate with destinate agent over signaling channel\n",
    "           say desired channel, exchange priority and schedule establish action\n",
    "           target: the target agent to communicate with\n",
    "        \"\"\"\n",
    "        score = priority()\n",
    "        target_score = target_agent.feedback(channel_index, score)\n",
    "        if score > target_score:\n",
    "            self.action_queue.append(self.establish)\n",
    "            self.args_queue.append((channel_index,))\n",
    "        # Since communicate operation need signaling bandwidth to coordinate\n",
    "        self.reward -= 1\n",
    "        \n",
    "    def feedback(self, channel_index, score):\n",
    "        \"\"\"schedule tear down action if score is higher\n",
    "           return the priority score of this agent\n",
    "        \"\"\"\n",
    "        if score > priority() and channel_index in self.channels:\n",
    "            self.action_queue.append(self.tear)\n",
    "            self.args_queue.append((channel_index,))\n",
    "        return priority()\n",
    "        \n",
    "    def priority(self):\n",
    "        \"\"\"calculate priority score, or loss of the agent\n",
    "           possible metric is to combine tasks num and self.reward\n",
    "           need to add constrict over score to avoid malicious deception\n",
    "           assume 通过（1）设备入网审查；（2）持续检测加入时间和后续发送数，保证score真实性\n",
    "        \"\"\"\n",
    "        return -self.task_num + self.reward\n",
    "    \n",
    "    def report(self):\n",
    "        return self.task_num, self.channels\n",
    "\n",
    "    def reward(self):\n",
    "        # TODO Try to get back reward from environment (receiver)\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python fastaiv1py37",
   "language": "python",
   "name": "fastaiv1py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
