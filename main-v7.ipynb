{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Option 1.1 channel_state: number of users\n",
    "# Option 1.2 channel_state: interference power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \"\"\"Currently no propogation effect is considered\n",
    "       The central station:\n",
    "       (1) only provide signaling channel and do not schedule at all\n",
    "       (2) monitor malicious users\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_num, max_steps):\n",
    "        \"\"\"channel_num: number of RF channels in the environment\n",
    "           max_steps: the number of steps in one epoch (ie. 50)\n",
    "                      when max_steps is reached game over and give reward\n",
    "           self.channel_state: log the channel user in this step\n",
    "           self.history: store channel occupancy of the past\n",
    "           self.step: log how many steps has gone\n",
    "           self.agent_list: log the agent in this environment\n",
    "           self.reward_list: record the reward of each agent\n",
    "        \"\"\"\n",
    "        self.channel_num = channel_num\n",
    "        self.channel_state = [[] for i in range(self.channel_num)]\n",
    "        self.history = []\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.step = 0\n",
    "        \n",
    "        self.agent_list = []\n",
    "        self.success_list = []\n",
    "        self.conflict_list = []\n",
    "        \n",
    "    def join(self, agent):\n",
    "        self.agent_list.append(agent)\n",
    "        self.success_list.append(0)\n",
    "        self.conflict_list.append(0)\n",
    "        \n",
    "    def one_action_step(self):\n",
    "        \"\"\"save channel state\n",
    "           enter to next time step and initialize channel state\n",
    "           call one_action_step method of all agents\n",
    "        \"\"\"\n",
    "        self.history.append(self.channel_state)\n",
    "        self.step += 1\n",
    "        if self.step > self.max_steps:\n",
    "            #output final result\n",
    "            self.report()\n",
    "            #visulize spectrum use and conflict\n",
    "            spec_his = np.array([[len(x) for x in y] for y in e.history])\n",
    "            plt.matshow(spec_his.T)\n",
    "            plt.colorbar()\n",
    "            raise RuntimeError('maximum steps reached and game over')\n",
    "        else:              \n",
    "            self.channel_state = [[] for i in range(self.channel_num)]\n",
    "            for agent in self.agent_list:\n",
    "                agent.one_action_step()\n",
    "            #evaluate the transmission of each agent\n",
    "            self.ber()\n",
    "    \n",
    "    def ber(self):\n",
    "        \"\"\"count the success task and conflict task\n",
    "        \"\"\"\n",
    "        for state in self.channel_state:\n",
    "            if len(state) == 1:\n",
    "                agent = state[0]\n",
    "                index = self.agent_list.index(agent)\n",
    "                self.success_list[index] += 1\n",
    "            if len(state) > 1:\n",
    "                for agent in state:\n",
    "                    index = self.agent_list.index(agent)\n",
    "                    self.conflict_list[index] += 1\n",
    "    \n",
    "    def broadcast(self):\n",
    "        return self.agent_list\n",
    "\n",
    "    def propagation(self, channel_index, agent):\n",
    "        \"\"\"propagate the signal of a certain agent\"\"\"\n",
    "        self.channel_state[channel_index].append(agent)\n",
    "\n",
    "    def query(self, channel_index):\n",
    "        \"\"\"return agents which occupied the channel\n",
    "           which can be implemented by signal classification in experiment\n",
    "        \"\"\"\n",
    "        return self.history[-1][channel_index]\n",
    "\n",
    "    def sense(self):\n",
    "        \"\"\"return the number of channel users\n",
    "           which can be implemented by spectrum sensing in experiment\n",
    "        \"\"\"\n",
    "        return [len(l) for l in self.history[-1]]\n",
    "\n",
    "    def get_reward(self, agent):\n",
    "        # The instructor is the environment (receiver),\n",
    "        # which can evaluate how good the agent is doing by checksum.\n",
    "        # The reward message is passed over signaling channel.\n",
    "        # Since the frequent receiver-to-agent interaction wastes bandwidth,\n",
    "        # the agent can only get back reward after its tasks are all finished\n",
    "        # or the maximum time step is reached.\n",
    "        if self.step < self.max_steps:\n",
    "            return 0\n",
    "        if self.step == self.max_steps:\n",
    "            index = self.agent_list.index(agent)\n",
    "            return self.success_list[index]+self.conflict_list[index]\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"report status of all agent\"\"\"\n",
    "        for index,agent in enumerate(self.agent_list):\n",
    "            print(\"success:{0:<4d}, \"\n",
    "                  \"conflict:{1:<4d}, \"\n",
    "                  \"remain:{2:<4d}, \"\n",
    "                  \"channel:{3:<3d}, \"\n",
    "                  \"reward: {4:<4.4f}\".format(\n",
    "                      self.success_list[index],\n",
    "                      self.conflict_list[index],\n",
    "                      agent.report()[0],\n",
    "                      len(agent.report()[1]),\n",
    "                      agent.report()[2],\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"at the beginning, random trasmit\n",
    "       then mainly use coordinate to gain channel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, task_num):\n",
    "        \"\"\"env: the environment to operate in\n",
    "           task_num: number of task to be transmitted, ie. 100\n",
    "           self.channels: established channels (coordinated with receiver)\n",
    "           self.part_state: observed channel states\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.task_num = task_num\n",
    "        self.channels = []\n",
    "        self.part_state = []\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.action_dict = {1:self.rest,\n",
    "                            2:self.transmit,\n",
    "                            3:self.establish,\n",
    "                            4:self.tear,\n",
    "                            5:self.observe,\n",
    "                            6:self.communicate}\n",
    "        self.action_queue = []\n",
    "        self.args_queue = []\n",
    "\n",
    "    def one_action_step(self):\n",
    "        \"\"\"interact with environment and agents within in one time step\n",
    "           using Q-learning to decide whether to tear channel when job finished\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.task_num > 0:\n",
    "            # job has not finished at last step, punish for the delay\n",
    "            # since we want to finish tasks as soon as possible\n",
    "            self.reward -= 0.5\n",
    "        \n",
    "        if len(self.action_queue) > 0:\n",
    "            # execute the actions scheduled by communication\n",
    "            for index, action in enumerate(self.action_queue):\n",
    "                action = self.action_queue[-1]\n",
    "                args = self.args_queue[index]\n",
    "                action(*args)\n",
    "            self.action_queue = []\n",
    "            self.args_queue = []\n",
    "        else:\n",
    "            # TODO: Q learning to choose operation (function and args)\n",
    "            # func: rest,transmit,establish,tear,observe,communicate\n",
    "            # args: channel_index\n",
    "            # channel_index: ie. 0~10\n",
    "            # status: task_num,channels,part_state\n",
    "            # find f: function, channel_index, target_agent_id = f(status)\n",
    "            \n",
    "            action_index = random.randint(1,6)\n",
    "            channel_index = random.randint(0,self.env.channel_num-1)\n",
    "            self.action_dict[action_index](channel_index)\n",
    "            #print(self.action_dict[action_index], channel_index)\n",
    "            \n",
    "        # get reward from environment\n",
    "        self.reward += self.env.get_reward(self)\n",
    "\n",
    "    def rest(self, *args):\n",
    "        \"\"\"reset in this time step and do nothing\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def transmit(self, *args):\n",
    "        \"\"\"transmit using all established channels\n",
    "           TODO: choose channel based on certain policy\n",
    "        \"\"\"\n",
    "        for index in self.channels:\n",
    "            if self.task_num == 0: return\n",
    "            self.env.propagation(index, self)\n",
    "            self.task_num -= 1       \n",
    "\n",
    "    def establish(self, channel_index, *args):\n",
    "        \"\"\"establish a new channel by coordinate with receiver\n",
    "           channel_index: the index of channel to occupy\n",
    "        \"\"\"\n",
    "        if channel_index not in self.channels:\n",
    "            self.channels.append(channel_index)\n",
    "            # Since expand operation need signaling bandwidth to coordinate\n",
    "            self.reward -= 1\n",
    "\n",
    "    def tear(self, channel_index, *args):\n",
    "        \"\"\"tear down a channel by coordinate with receiver\n",
    "           channel_index: the index of channel to release\n",
    "        \"\"\"\n",
    "        if channel_index in self.channels:\n",
    "            self.channels.remove(channel_index)\n",
    "            # Shrink operation do not need signaling bandwidth to coordinate (0)\n",
    "            # use the current data channel to trasmit tear down signal\n",
    "            # Shrink operation ficilitate collabaration(+1)\n",
    "            # decrease the possibility to conflict\n",
    "            self.reward += 1\n",
    "\n",
    "    def observe(self, *args):\n",
    "        \"\"\"observe the channel usages\n",
    "           TODO: may directly return the channel with highest availability\n",
    "           TODO: in this stage, we let Q-learning find the candidate channel\n",
    "        \"\"\"\n",
    "        self.part_state.append(self.env.sense()) \n",
    "        # Since observe operation need energy to detect occupancy\n",
    "        self.reward -= 0.2\n",
    "        # TODO find the channel with highest availability (least occupied)\n",
    "    \n",
    "    # abandon p2p communicate\n",
    "    # reason 1: p2p communicate -> feedback are not stable\n",
    "    # ie. A:0.1, B:0.3, C:0.2, multiple communicate may overwrite each other\n",
    "    # reason 2: identify result is not reliable\n",
    "    # since you do not exactly know whether the agent is still using the channel\n",
    "    # so use broadcast communicate\n",
    "    def communicate(self, channel_index, *args):\n",
    "        \"\"\"communicate with other agents over signaling channel\n",
    "           the priority and desired channel is broadcast to all other agents           target: the target agent to communicate with\n",
    "           protocal: (1) exchange priority score to collabration\n",
    "                     (2) protect agents using different protocals\n",
    "           since can not collaborate with them means can not use the channel\n",
    "           and you will be interferenced when you do\n",
    "        \"\"\"\n",
    "        # Since communicate need signaling bandwidth to coordinate (-1)\n",
    "        self.reward -= 1\n",
    "        score = self.priority()\n",
    "        for agent in self.env.broadcast():\n",
    "            if not agent.insist(channel_index, score): break\n",
    "        else:\n",
    "            return\n",
    "        self.action_queue.append(self.establish)\n",
    "        self.args_queue.append((channel_index,))\n",
    "        \n",
    "    # TODO: log communicate result and use Q-learning decide what to do next\n",
    "    # Infeasible： since different agents are difficult to reach consensus\n",
    "\n",
    "    def insist(self, channel_index, score):\n",
    "        \"\"\"schedule tear down action if score is higher\n",
    "           return the priority score of this agent\n",
    "        \"\"\"\n",
    "        if channel_index not in self.channels:\n",
    "            return True\n",
    "        if score > self.priority():\n",
    "            self.action_queue.append(self.tear)\n",
    "            self.args_queue.append((channel_index,))\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def priority(self):\n",
    "        \"\"\"calculate priority score, or loss of the agent\n",
    "           possible metric is to combine tasks num and self.reward\n",
    "           need to add constrict over score to avoid malicious deception\n",
    "           通过（1）设备入网审查；（2）监测加入时间和后续发送数，保证score真实性\n",
    "        \"\"\"\n",
    "        return self.task_num - len(self.channels) - self.reward\n",
    "    \n",
    "    def report(self):\n",
    "        return self.task_num, self.channels, self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize environment\n",
    "e = Environment(50, 500)\n",
    "\n",
    "# add agent into environment\n",
    "for i in range(20):\n",
    "    e.join(Agent(e, 1000))\n",
    "\n",
    "#start runing\n",
    "for i in range(501):\n",
    "    try:\n",
    "        e.one_action_step()\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in e.agent_list:\n",
    "    print(a)\n",
    "    print(a.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(object):\n",
    "    def __init__(self):\n",
    "        A.a = 1\n",
    "        A.d = {1:self.b, 2:self.c}\n",
    "        \n",
    "    def b(self):\n",
    "        print('b')\n",
    "        self.c()\n",
    "    \n",
    "    def c(self):\n",
    "        print('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = A()\n",
    "a.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"wordl {0:<10.4f} hell\".format(1.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python fastaiv1py37",
   "language": "python",
   "name": "fastaiv1py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
